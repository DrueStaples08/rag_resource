{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Langchain and vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data injection techniques\n",
    "- TextLoader: load text files (.txt)\n",
    "- WebLoader: load, chunk, and index scraped data from a html page (e.g. BeautifulSoup)\n",
    "- PDFLoader: Load data from a pdf file (.pdf)\n",
    "\n",
    "LTE\n",
    "- Load\n",
    "- Transfrom\n",
    "- Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents \n",
    "text_loader = TextLoader(\"example_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_docs = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'example_text.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call API Keys\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sort-by-commitment posting-category medium-category-label capitalize-labels commitment'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web Loader: loads data from html page\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_path=\"https://jobs.lever.co/egen/0e0d9469-08a8-48aa-aefb-f65d50760e80\",\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "        # class names from HTML Page\n",
    "        class_=(\n",
    "            \"sort-by-time posting-category medium-category-label width-full capitalize-labels location\",\n",
    "            \"sort-by-team posting-category medium-category-label capitalize-labels department\",\n",
    "            \"sort-by-commitment posting-category medium-category-label capitalize-labels commitment\"\n",
    "            )\n",
    "        ))\n",
    "    )\n",
    "\n",
    "\"sort-by-commitment posting-category medium-category-label capitalize-labels commitment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://jobs.lever.co/egen/0e0d9469-08a8-48aa-aefb-f65d50760e80'}, page_content='Naperville, IL or RemoteEngineering and Product Management /Full Time /')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDFLoader, Chunking, and Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: PyPDFLoader will actually chunk everything by page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF Data\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_loader = PyPDFLoader(\"robot.pdf\")\n",
    "pdf_loader = PyPDFLoader(\"Profile.pdf\")\n",
    "# pdf_loader = PyPDFLoader(\"movie_theater.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Profile.pdf', 'page': 0}, page_content=\"\\xa0 \\xa0\\nContact\\ndruestaples@gmail.com\\nwww.linkedin.com/in/drue-\\nstaples-65b206182  (LinkedIn)\\nTop Skills\\nPython (Programming Language)\\nTensorFlow\\nDeep LearningDrue Staples\\nLead AI Machine Learning Engineer at PROLIFIC AI\\nLowell, Indiana, United States\\nSummary\\nSpecific Skills:\\nPython, Pandas, Scikit-Learn, Matplotlib, Numpy, Tensorflow,\\nOpenCV, Keras, Seaborn, Statistics, Probability, Differential\\nCalculus, Linear Algebra, SQL, Machine Learning Algorithms,\\nFeature Engineering, Feature Selection, Google Cloud Platform,\\nNLP, Jupyter, Excel, Powerpoint, Word, MATLAB, Speech\\nRecognition, Sentiment Analysis, Time Series data, Agile\\nMethodology, HTML, CSS, JavaScript, PHP\\nCertification:\\nStanford's Machine Learning Certification with Andrew Ng\\nCourse includes machine learning, datamining, and statistical pattern\\nrecognition. Topics include: (i) Supervised learning (parametric/non-\\nparametric algorithms, support vector machines, kernels, neural\\nnetworks). (ii) Unsupervised learning (clustering, dimensionality\\nreduction, recommender systems, deep learning). (iii) Best practices\\nin machine learning (bias/variance theory; innovation process in\\nmachine learning and AI).\\nExperience\\nPROLIFIC AI\\nLead AI Machine Learning Engineer\\nJuly 2023\\xa0-\\xa0Present\\xa0 (1 year)\\nBuilding and leading a team of scientists and engineers to design, develop,\\nand deploy the Minimum Viable Product. PROLIFIC is an AI-based rideshare\\ncompany that is focused on enhancing travel of sporting events with the\\nuse of machine learning. This entails a mobile app that allows users to\\nview upcoming games with a confidence score of how much fulfillment\\n\\xa0 Page 1 of 4\"),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 1}, page_content='\\xa0 \\xa0\\nthey are expected to receive. This also includes a list of mobile games and\\nfeatures which will be trained by a set of models to ensure user satisfaction/\\nengagement. Once a user books a ride to an event, they can connect their\\napp to the PROLIFIC vehicle’s touchscreens to access their app with even\\nmore capabilities. This includes sports statistics, highlights,and games that will\\ninclude its own recommendation system. Current tools I’m using for this project\\nare Google Cloud Platform, Python, TensorFlow, SQL, Kubeflow, GIT/GitHub,\\nDocker, Kubernetes, Seldon Core, HTML, CSS, JavaScript, and React Native \\nAIM Consulting Group\\nData Scientist/ AI ML Engineer\\nOctober 2022\\xa0-\\xa0August 2023\\xa0 (11 months)\\nDesign, build, and analyze production ready ML models on the Search Team\\nfor Best Buy.\\nODEM.IO\\nMachine Learning Engineer\\nApril 2022\\xa0-\\xa0November 2022\\xa0 (8 months)\\nCreate and integrate ML for the Startup ODEM Platform which will enable\\nusers to find more accurate job matches, desired education/career  pathways,\\nand earn college credits with real world experience. \\nPlainsight\\nMachine Learning Engineer\\nJanuary 2022\\xa0-\\xa0April 2022\\xa0 (4 months)\\nEngineering Machine Learning Solutions with Computer Vision and the Sense\\nAI Platform\\nProjects include a wok weight regressor for Panda Express and cow counter\\nfor a farm \\nApplied Research Solutions\\nArtificial Intelligence Machine Learning Developer\\nApril 2020\\xa0-\\xa0January 2022\\xa0 (1 year 10 months)\\nMachine Learning Scientist for the US Air Force Research Laboratory.\\nWorked on three projects and wrote 2 papers, 1 of which was published.\\nProjects include topics around:\\n-Topological Data Analysis \\n-Reinforcement Learning \\n-Analogical Reasoning \\n\\xa0 Page 2 of 4'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 2}, page_content='\\xa0 \\xa0\\nVision13\\nArtificial Intelligence Software Engineer\\nSeptember 2019\\xa0-\\xa0April 2020\\xa0 (8 months)\\nConceptualize, design,build and modify Artificial Intelligence Software, and\\npractice business development principles for an AI automated stock trading\\napplication. Tasks include building the model, data extraction via realtime\\nmediums, data preprocessing, speech recognition and sentiment analysis.\\nFreelancer\\nData Analyst/ Machine Learning Engineer\\nSeptember 2017\\xa0-\\xa0September 2019\\xa0 (2 years 1 month)\\nNorthwest Indiana\\n-Python Programming\\nMy open-source projects (can be viewed on Github) include Python and I also\\nhave a Python Tutorial Series on YouTube where I teach the fundamentals of\\nthe Python Language. A part of that, I also actively engage in online Python,\\nData Science, Machine Learning, and Deep Learning social communities.\\n-Data Analysis\\nAbility to create insight and easy visualization. One project of mine displays\\nthe countries that have the highest population growth from the past 55 years.\\nAnother project showcases top characters in a 23 year-old game. The code for\\nthese programs can be seen on my Github as well as YouTube.\\n-Machine Learning | Deep Learning\\nComprehensive with various ML libraries and Frameworks. Some projects can\\nbe listed as follows: \\nBreast Cancer Detector - Detects whether tumors are malignant or benign \\nTemperature Transformer - Converts Celsius to Fahrenheit \\nFarm Project - Find similarities between farm crops \\nClothing Project - Detect different types of clothing \\nMPG Project - Predict the miles per gallon of discrete vehicles\\nEducation\\nValparaiso University\\nElectrical and Electronics Engineering\\nStanford University\\nCoursera’s Machine Learning Program,\\xa0ML \\xa0·\\xa0(2019\\xa0-\\xa02019)\\n\\xa0 Page 3 of 4'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 3}, page_content='\\xa0 \\xa0\\n\\xa0 Page 4 of 4')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk Documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the documents have already been chunked by page number, there is still room to further chunk the text into even smaller pieces.\n",
    "\n",
    "This helps with:\n",
    "- indexing and retrieval as smaller chunks are easier to process\n",
    "- increasing granularity (scale or level of detail) as a page might include multiple topics\n",
    "- preserving context with the chunk_overlap parameter below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is chunk size and chunk overlap? (RecursiveCharacterTextSplitter)\n",
    "\n",
    "- Chunk size is just the number of characters to be inlcluded in each chunk \n",
    "- Chunk overlap: the last number of characters to be used from the previous chunk. This helps the model better understand the context of the corpus. \n",
    "- Example 1:\n",
    "    - Chunk 0: 10 characters \n",
    "    - Chunk 1: 5 last characters from previous chunk 0 + 5 new characters\n",
    "    - Chunk 2: 5 last characters from previous chunk 1 + 5 new characters\n",
    "    - Chunk 3: 5 last characters from previous chunk 2 + 5 new characters\n",
    "- Example 2: input=\"The dog ran up the hill.\", chunk_size=10, chunk_overlap=5\n",
    "    - Chunk 0: \"The dog ra\"\n",
    "    - Chunk 1: \"dog ran up\"\n",
    "    - Chunk 2: \"ran up the\"\n",
    "    - Chunk 3: \"up the hil\"\n",
    "    - Chunk 4: \"the hill.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk documents  (Transform)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=20, chunk_overlap=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs_split = text_splitter.split_documents(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Profile.pdf', 'page': 0}, page_content='Contact'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 0}, page_content='druestaples@gmail.c'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 0}, page_content='es@gmail.com')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs_split[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB with Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into vector embeddings (Chroma)\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to vector store (Chroma)\n",
    "db = Chroma.from_documents(documents=pdf_docs_split[:10], embedding=OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x137fd6df0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the vector database\n",
    "# query = \"What projects are with GCP\"\n",
    "query = \"What the top rated skills?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.similarity_search(query=query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'Profile.pdf'}, page_content='(LinkedIn)'),\n",
       " Document(metadata={'page': 0, 'source': 'Profile.pdf'}, page_content='Top Skills'),\n",
       " Document(metadata={'page': 0, 'source': 'Profile.pdf'}, page_content='es@gmail.com')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.similarity_search_by_vector(embedding=embedding, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(LinkedIn)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0].dict()['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(LinkedIn)'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS Vector Store with Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Vector Database\n",
    "\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = FAISS.from_documents(documents=pdf_docs_split, embedding=OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x29b457b50>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the vector database (FAISS)\n",
    "# query = \"What projects are with GCP\"\n",
    "query = \"What the top rated skills?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Profile.pdf', 'page': 0}, page_content='(LinkedIn)'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 0}, page_content='Summary'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 1}, page_content='Data Analysis'),\n",
       " Document(metadata={'source': 'Profile.pdf', 'page': 2}, page_content='-Data Analysis')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lance Vector Store with Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lance vector database\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example_rag_vm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
